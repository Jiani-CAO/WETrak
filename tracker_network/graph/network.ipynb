{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa7d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:\\\\WETrak\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Callable, Optional\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1cbeb",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87bbdadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,channel):\n",
    "        super(Encoder,self).__init__()\n",
    "\n",
    "        self.down_block1 = nn.Sequential(nn.Conv2d(1, channel, 3, padding=1), \n",
    "                                         nn.BatchNorm2d(channel), \n",
    "                                         nn.ReLU(True), \n",
    "                                         nn.Dropout(p=0.05), \n",
    "                                         nn.MaxPool2d((5,1), stride=(5,1)))\n",
    "        \n",
    "        self.down_block2 = nn.Sequential(nn.Conv2d(channel, channel*4, 3, padding=1), \n",
    "                                         nn.BatchNorm2d(channel*4), \n",
    "                                         nn.ReLU(True), \n",
    "                                         nn.Dropout(p=0.05), \n",
    "                                         nn.MaxPool2d((4,1), stride=(4,1)))\n",
    "        \n",
    "        self.down_block3 = nn.Sequential(nn.Conv2d(channel*4, channel*8, 3, padding=1), \n",
    "                                         nn.BatchNorm2d(channel*8), \n",
    "                                         nn.ReLU(True), \n",
    "                                         nn.Dropout(p=0.05), \n",
    "                                         nn.MaxPool2d((2,1), stride=(2,1)))\n",
    "        \n",
    "        # initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self,x):# \n",
    "        # downsample\n",
    "        x = self.down_block1(x) # [batch, 32, 200, 1]\n",
    "        x = self.down_block2(x) # [batch, 128, 50, 1]\n",
    "        x = self.down_block3(x) # [batch, 256, 25, 1]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb01c17",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b22963b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "        \n",
    "        # initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "\n",
    "class ChannelPool(nn.Module): # cat([B,1,H,W],[B,1,H,W]) -> [B,2,H,W]\n",
    "    def forward(self, x):\n",
    "        out = torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,time_step,finger_type):\n",
    "        super(Attention, self).__init__()\n",
    "        self.finger_type = finger_type\n",
    "        \n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Flatten(),\n",
    "                                nn.Linear(time_step*5,5))\n",
    "        \n",
    "        # initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x) #[B,2,H,W]\n",
    "        x_out = self.spatial(x_compress) #[B,1,H,W]\n",
    "        \n",
    "        x_out = self.fc(x_out) # [B,W]\n",
    "        scale = F.softmax(x_out,dim=1).unsqueeze(1).unsqueeze(2) # [B,1,1,W]\n",
    "        \n",
    "        return x * scale\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.weight, 0)\n",
    "                \n",
    "                if self.finger_type == ['Pinky']:\n",
    "                    m.bias = nn.parameter.Parameter(torch.Tensor((1,0,0,0,1)))\n",
    "                    \n",
    "                elif self.finger_type == ['Thumb']:\n",
    "                    m.bias = nn.parameter.Parameter(torch.Tensor((0,0,1,1,0)))\n",
    "                    \n",
    "                elif self.finger_type == ['Index','Middle','Ring']:\n",
    "                    m.bias = nn.parameter.Parameter(torch.Tensor((0,1,0,0,1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2ffa3",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a31006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef7e0e4",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06dc7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,finger_type,is_norm,channel=32):\n",
    "        super(Decoder,self).__init__()\n",
    "\n",
    "        self.up_blcok1 = nn.Sequential(nn.Conv2d(channel*8, channel*4, 3, padding=1), \n",
    "                                       nn.BatchNorm2d(channel*4), \n",
    "                                       nn.ReLU(True), \n",
    "                                       nn.Dropout(p=0.05), \n",
    "                                       nn.Upsample(scale_factor=(5,2)))\n",
    "        \n",
    "        self.up_blcok2 = nn.Sequential(nn.Conv2d(channel*4, channel, 3, padding=1), \n",
    "                                       nn.BatchNorm2d(channel), \n",
    "                                       nn.ReLU(True), \n",
    "                                       nn.Dropout(p=0.05), \n",
    "                                       nn.Upsample(scale_factor=(4,2)))\n",
    "        \n",
    "        if is_norm == True:\n",
    "            self.up_blcok3 = nn.Sequential(nn.Conv2d(channel, 1, 3, padding=1), \n",
    "                                           nn.BatchNorm2d(1), \n",
    "                                           nn.Sigmoid(), \n",
    "                                           nn.Dropout(p=0.05), \n",
    "                                           nn.Upsample(scale_factor=(2,len(finger_type))))\n",
    "            \n",
    "        elif is_norm == False:\n",
    "            self.up_blcok3 = nn.Sequential(nn.Conv2d(channel, 1, 3, padding=1), \n",
    "                                           nn.BatchNorm2d(1), \n",
    "                                           nn.ReLU(True), \n",
    "                                           nn.Dropout(p=0.05), \n",
    "                                           nn.Upsample(scale_factor=(2,len(finger_type))))\n",
    "        \n",
    "        # initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # upsample\n",
    "        x = self.up_blcok1(x) # [batch, 128, 125, 4]\n",
    "        x = self.up_blcok2(x) # [batch, 32, 500, 4]\n",
    "        x = self.up_blcok3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd64aceb",
   "metadata": {},
   "source": [
    "### Tracker Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68adaa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,finger_type,is_norm,emg_channel=5,time_step=25,channel=32,is_att=True):\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.emg_channel = emg_channel\n",
    "        self.is_att = is_att\n",
    "        \n",
    "        # downsample\n",
    "        self.encoder = Encoder(channel)\n",
    "        \n",
    "        # w/0 attention\n",
    "        self.max_pool = nn.MaxPool2d((1,emg_channel), stride=(1,emg_channel))\n",
    "        \n",
    "        # Resnet\n",
    "        self.resnet = Bottleneck(inplanes=channel*8,planes=channel*8)\n",
    "        # upsample\n",
    "        self.decoder = Decoder(finger_type,is_norm)\n",
    "        \n",
    "        # initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # attention\n",
    "        self.attention = Attention(time_step,finger_type)\n",
    "        \n",
    "    def forward(self,x):# x:[1,1,1000,5or2]\n",
    "        # downsample\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # w/o attention\n",
    "        if self.is_att == False:\n",
    "            x = self.max_pool(x)\n",
    "        # attention\n",
    "        elif self.is_att == True:\n",
    "            x = self.attention(x)\n",
    "            x = self.max_pool(x)\n",
    "        \n",
    "        # resnet\n",
    "        x = self.resnet(x)\n",
    "        x = self.resnet(x)\n",
    "        x = self.resnet(x)\n",
    "        x = self.resnet(x)\n",
    "        x = self.resnet(x)    \n",
    "        \n",
    "        # upsample\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
